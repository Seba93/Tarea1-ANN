{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2. Back-propagation (BP) from Scratch <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se implementará el algoritmo Back-propagation (de ahora en adelante, BP), frecuentemente utilizado en el entrenamiento de redes neuronales *feedforward*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.0 Importaciones necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import pandas\n",
    "import matplotlib\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Entrenamiento de red *feedforward* vía BP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.1 Inicialización de la red**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, es necesario definir el tipo de dato que se utilizará para implementar la red, así como también el de cada de uno de sus componentes (capas y neuronas). Cada neurona será un diccionario que almacena los pesos asociados a ella. Luego, cada capa será implementada como un arreglo de neuronas y a su vez, la red completa será un arreglo de capas. Se considerará que cada red posee sólo una capa oculta.\n",
    "\n",
    "Por otro lado, sea I la dimensionalidad de cada input, H el número de neuronas de la capa oculta y O el número de neuronas de la capa oculta (o en otras palabras, la cantidad de clases existentes en el problema de clasificación respectivo). Teóricamente, a cada neurona de la capa oculta se le asocian I + 1 pesos, por lo que dicha capa tiene un total de H x (I + 1) pesos asociados. Además, a cada neurona de la capa de salida le corresponden H + 1 pesos, sumando un total de O x (H + 1) pesos para esta capa. Notar que se ha sumado 1 en cada caso para considerar el *bias*.\n",
    "\n",
    "De esta manera, se procede a implementar la función *init_network*, que recibe a I, H y O como parámetros, y genera como salida una red *feedforward* con una capa oculta y una capa de salida, compuestas por H y O neuronas, respectivamente. Los pesos son determinados de forma aleatoria con valores entre 0 y 1. Por simplicidad, el *bias* es incluído como un peso más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_network(I, H, O):\n",
    "    # Se inicializa red feedforward vacía\n",
    "    ff_network = []\n",
    "    # Se implementa capa oculta, compuesta por H neuronas e H x (I + 1) pesos \n",
    "    hidden_layer = [{'weights': [random.random() for i in range (I + 1)]} for j in range (H)]\n",
    "    # Se implementa capa de salida, compuesta por O neuronas y O x (H + 1) pesos\n",
    "    output_layer = [{'weights': [random.random() for i in range (H + 1)]} for j in range (O)]\n",
    "    # Se agregan las capas a la red\n",
    "    ff_network.extend([hidden_layer, output_layer])\n",
    "    return ff_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.2 Implementación de *forward pass***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se implementará lo que se conoce como *forward-propagation*. Se denomina *forward-propagation* al proceso de generar una respuesta a partir de la propagación de un input determinado a través de las capas de una red neuronal artificial. Dicho proceso es vital en la fase de entrenamiento de la red, pues posibilita que los pesos de cada neurona sean modificados en base al output generado. De forma análoga, será vital para la realización de pruebas sobre inputs no conocidos en la sección 2.3.\n",
    "\n",
    "Para realizar este procedimiento, primero es necesario determinar la **activación de cada neurona**, que corresponde a la suma ponderada de las variables asociadas a un determinado input en base a su peso. Luego, se implementa la función *neuron_activation* que, a partir de un determinado input y sus pesos, determina el valor de la activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_activation(input_, weights):\n",
    "    # Se inicializa activación en 0\n",
    "    activation = 0\n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += weights[i] * input_[i]\n",
    "    # Se suma bias\n",
    "    activation += weights[-1]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de que cada neurona es activada, es necesario **transferir la activación**, esto es, aplicar una función no lineal sobre la activación obtenida previamente. Por tanto, se debe escoger una función de este tipo para realizar la transferencia. En este caso, se utilizará la función *tanh* para ambas capas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo esta función implementada, se está en condiciones de realizar el forward pass. Para ello, se implementa la función *forward pass*, que recibe la red completa y un input determinado como parámetros. Así, por cada capa, se calcula el output producido por cada una de las neuronas que forman parte de ella, de tal forma que los outputs de una capa corresponden a los inputs de la siguiente. En este caso, los outputs de la capa oculta son los inputs de la capa de salida. Recordar que hasta este momento, las neuronas están almacenando los pesos que se le asocian. Gracias a esta función, cada neurona también almacenará el output que produce a partir del input y de los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(network, input_):\n",
    "    for layer in network:\n",
    "        output = []\n",
    "        for neuron in layer:\n",
    "            # Se realiza activación de cada neurona\n",
    "            activation = neuron_activation(input_, neuron['weights'])\n",
    "            # Se realiza transferencia por medio de función ReLU\n",
    "            neuron['output'] = math.tanh(activation)\n",
    "            output.append(neuron['output'])\n",
    "            # Los outputs de la capa oculta corresponden a los inputs de la capa de salida\n",
    "        input_ = output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.3 Implementación de *backward pass***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenidos los outputs a partir de un input determinado, es necesario calcular el error asociado a cada uno de los outputs individuales. A continuación, estos errores son \"informados\" a la capa oculta por medio de la capa de salida, de tal manera que los pesos asociados a cada neurona son recalculados.\n",
    "\n",
    "Para realizar este proceso, primero se requiere **evaluar la derivada de la función de transferencia sobre los outputs**. La derivada de la función *tanh* es:\n",
    "\n",
    "**f'(x) = 1 - tanh<sup>2</sup>(x)**\n",
    "\n",
    "Luego, se procede a implementar la derivada de esta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementación de la derivada de la función tanh\n",
    "def tanh_derivative(output):\n",
    "    return 1 - math.tanh(output) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En seguida, se requiere **definir como se calculará el error asociado a cada neurona de la capa oculta**. Para esto, primero es necesario calcular el error asociado a cada neurona presente en la capa de salida, el que puede definirse como:\n",
    "\n",
    "**error_neurona_capa_salida = (valor_real - valor_calculado) x f'(valor_calculado)**\n",
    "\n",
    "Sin embargo, la forma de calcular el error asociado a cada neurona en la capa oculta es diferente. Dicho error se obtiene a partir de la suma ponderada de los errores de las neuronas de la capa de salida por sus pesos, es decir:\n",
    "\n",
    "**error_neurona_capa_oculta_i = sumatoria (peso_ij x error_neurona_capa_salida_j) x f'(valor_calculado)**\n",
    "\n",
    "Con esta idea en mente, se implementa la función *backward_pass*, que recibe la red completa y un arreglo con los valores esperados. Lo que hace esta función es almacenar en cada neurona presente en la red el error calculado para ella (en términos de código, esto implica adicionar una nueva llave al diccionario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_pass(network, expected_values):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = []\n",
    "        # Primero se computan los errores en la capa de salida\n",
    "        if i == len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected_values[j] - neuron['output'])\n",
    "        \n",
    "        # Luego, se calcula el error de cada neurona de la capa oculta a partir de los errores de las neuronas de la capa\n",
    "        # de salida\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0\n",
    "                # Se realiza la suma ponderada sobre las neuronas de la capa de salida\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += neuron['weights'][j] * neuron['error']\n",
    "                errors.append(error)\n",
    "        \n",
    "        # Para cada capa, se multiplica el error de cada neurona por la derivada de la función de transferencia evaluada\n",
    "        # en el output de dicha neurona\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['error'] = errors[j] * tanh_derivative(neuron['output'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.4 Entrenamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estando implementados tanto el *backward pass* como el *forward pass*, es posible dar paso a la construcción del proceso de entrenamiento del modelo. En este caso, se utilizará Gradiente Descendente Estocástico (de ahora en adelante, SGD) para ello. Dicho proceso involucra la ejecución de múltiples iteraciones. En cada una, los ejemplos son propagados a través de la red, para luego \"devolver\" los errores correspondientes a cada neurona y así poder actualizar los pesos de cada una. Un peso determinado es actualizado de la siguiente forma:\n",
    "\n",
    "**peso_i = peso_i-1 + tasa_de_aprendizaje x error_i-1 x input**\n",
    "\n",
    "Donde *peso_i* es el nuevo peso de la neurona, *peso_i-1* es su peso actual, *error_i-1* corresponde al error asociado a cada neurona por medio del *backward pass* e *input* es el valor que causó el error. Por otra parte, *tasa_de_aprendizaje* es un parámetro que permite controlar que tanto influye el error asociado a una neurona en la actualización de sus pesos y puede interpretarse como una medida de que tan rápido aprende el modelo, pues mientras mayor es esta tasa, mayor es el cambio que experimentan los pesos.\n",
    "\n",
    "Notar que los pesos son actualizados por medio de un **método iterativo**.\n",
    "\n",
    "Es importante tener en cuenta lo que ocurre con los *bias*. Como no están conectados a ninguna variable del input, sólo se considera la tasa de aprendizaje y el error, de tal forma que:\n",
    "\n",
    "bias_i = bias_i-1 + tasa_de_aprendizaje x error_i-1\n",
    "\n",
    "Así, se procede a implementar la función *update_weights*, la cual se encargará de actualizar los pesos de la red. Esta función recibe la red completa, la tasa de aprendizaje y un input determinado como parámetros obligatorios. Como parámetro optativo, está *weight_decay*, que por ahora no será utilizado (ver sección 2.4 para más detalle). Como salida, se entrega la red con sus pesos actualizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se actualizan pesos de la red\n",
    "def update_weights(network, learning_rate, input_, weight_decay=None):\n",
    "    for i in range(len(network)):\n",
    "        # Pesos y bias son actualizados de forma diferente\n",
    "        input_ = input_[:-1]\n",
    "        # Los outputs de la capa oculta corresponden a los inputs de la capa de salida\n",
    "        if i != 0:\n",
    "            input_ = [neuron['output'] for neuron in network[i-1]]\n",
    "            \n",
    "        # Por cada capa...\n",
    "        for neuron in network[i]:\n",
    "            # ... se actualizan pesos de cada neurona...\n",
    "            for j in range(len(input_)):\n",
    "                if weight_decay:\n",
    "                    neuron['weights'][j] += learning_rate * neuron['error'] * input_[j] - learning_rate * weight_decay * \\\n",
    "                        neuron['weights'][j]\n",
    "                else:\n",
    "                    neuron['weights'][j] += learning_rate * neuron['error'] * input_[j]\n",
    "            # Y finalmente el bias\n",
    "            neuron['weights'][-1] += learning_rate * neuron['error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, se está en condiciones de implementar el entrenamiento de la red, proceso que basicamente consistirá en realizar, sobre cada ejemplo perteneciente a un determinado conjunto de entrenamiento, un *forward pass*, un *backward pass* y actualización de pesos, siendo este proceso repetido tantas veces como iteraciones (en este contexto, *épocas*) se desee. Luego, se procede a implementar la función *train_network*, que lleva a cabo el proceso previamente descrito. Esta función recibe una red inicializada aleatoriamente, un conjunto de entrenamiento, tasa de aprendizaje, número de épocas, número de clases del problema como parámetros obligatorios. Como parámetro optativo, está *weight_decay*, no utilizado por ahora. Como salida, se entrega una red entrenada. Notar que a cada ejemplo le corresponde un vector binario que indica su clase, de tal forma que si pertenece a la clase i, este vector tendrá un 1 en la posición y ceros en las posiciones restantes. Esto es necesario para poder comparar el vector de outputs generado por la capa de salida con los valores esperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se entrena una red determinada\n",
    "def train_network(network, train_dataset, learning_rate, n_epochs, n_classes, weight_decay=None):\n",
    "    for epoch in range(n_epochs):\n",
    "        # Para cada ejemplo del conjunto de entrenamiento...\n",
    "        for example in train_dataset:\n",
    "            # ... se realiza, en primer lugar, el forward pass\n",
    "            output = forward_pass(network, example)\n",
    "            # se define vector binario asociado al ejemplo\n",
    "            example_class = int(example[-1] - 1)\n",
    "            expected_values = [0 for i in range(n_classes)]\n",
    "            expected_values[example_class] = 1\n",
    "            # Se lleva a cabo backward pass\n",
    "            backward_pass(network, expected_values)\n",
    "            # Se actualizan los pesos de la red\n",
    "            # Se utiliza weight decay, en caso de que este parametro sea especificado\n",
    "            update_weights(network, learning_rate, example, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Realizando predicciones sobre una red *feed forward***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se implementa la función *predict_classes*, cuyo propósito es realizar predicciones sobre un conjunto de pruebas. Para ello, se hará uso de la función *forward_pass*. Se sabe que esta función genera un vector de dimensionalidad igual al número de clases del problema y que el elemento i-ésimo de dicho vector corresponde a la probabilidad de que el ejemplo propagado a través de la red de pertenezca a la clase i. Luego, el ejemplo pertenecerá a la clase que le corresponda la mayor probabilidad.\n",
    "\n",
    "*predict_classes* recibe una red previamente entrenada y un conjunto de pruebas como parámetros. Como salida, entrega el error de clasificación promedio obtenido sobre el conjunto de pruebas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se implementa prediccion sobre ejemplos de prueba\n",
    "def predict_classes(network, test_dataset):\n",
    "    # Se lleva un conteo de ejemplos mal clasificados\n",
    "    misclassified_examples = 0.0\n",
    "    for example in test_dataset:\n",
    "        # Se generan probabilidades\n",
    "        output = forward_pass(network, example)\n",
    "        # Se obtiene clase asociada a la mayor probabilidad\n",
    "        example_predicted_class = output.index(max(output)) + 1\n",
    "        # Se verifica si el ejemplo fue bien o mal clasificado\n",
    "        if example_predicted_class != example[-1]:\n",
    "            misclassified_examples += 1\n",
    "    return misclassified_examples / len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Aplicación sobre problema real**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se busca comprobar el funcionamiento de los algoritmos implementados sobre un dataset real, *seeds*, utilizado para la clasificación de semillas. El dataset está constituido por 210 ejemplos. Además, cada ejemplo está definido por 7 atributos y se definen 3 clases de semilla. Los datos serán normalizados previo a su utilización.\n",
    "\n",
    "El dataset original es dividido en 168 ejemplos de entrenamiento (80% del dataset) y 42 ejemplos de prueba (20% del dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se descarga y almacena dataset en un dataframe\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt'\n",
    "df = pandas.read_csv(url, sep=r'\\s+', header=None)\n",
    "# Se normaliza dataframe\n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(df.ix[:, 0:6])\n",
    "df.ix[:, 0:6] = scaler.transform(df.ix[:, 0:6])\n",
    "# Se convierte el dataset a formato lista, el cual es utilizado por las funciones implementadas en las secciones previas\n",
    "dataset = df.values.tolist()\n",
    "# Se genera conjunto de entrenamiento\n",
    "train_dataset = dataset[:int(math.floor(0.8 * len(dataset)))]\n",
    "# Se genera conjunto de pruebas\n",
    "test_dataset = dataset[int(math.floor(0.8 * len(dataset))):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generados el conjunto de entrenamiento y el conjunto de pruebas, se procede a entrenar la red sobre el conjunto de entrenamiento, para luego realizar predicciones sobre este y el conjunto de pruebas. Se usará una tasa de aprendizaje de 0.03 y una cantidad variable de épocas (desde 10 hasta 100, aumentando de 5 en 5). La capa oculta de la red estará constituida por 10 neuronas, mientras que la capa de salida por 3 neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se define tasa de aprendizaje\n",
    "learning_rate = 0.03\n",
    "# Se define cantidad maxima de épocas\n",
    "max_epochs = 100\n",
    "\n",
    "# Se obtiene cantidad de atributos de cada ejemplo (en este caso, 7)\n",
    "n_features = len(train_dataset[0]) - 1\n",
    "# Se obtiene cantidad de clases (en este caso, 3)\n",
    "n_classes = len(set([row[-1] for row in train_dataset]))\n",
    "# Se inicializa red\n",
    "random.seed(1)\n",
    "network = init_network(n_features, 10, n_classes)\n",
    "\n",
    "epochs = []\n",
    "# Se almacenan errores para cada número de épocas posible\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Para cada cantidad de épocas posible...\n",
    "for i in range(10, max_epochs + 5, 5):\n",
    "    # ... se entrena la red...\n",
    "    train_network(network, train_dataset, learning_rate, i, n_classes)\n",
    "    # ... y se obtiene error promedio\n",
    "    train_error = predict_classes(network, train_dataset)\n",
    "    test_error = predict_classes(network, test_dataset)\n",
    "    epochs.append(i)\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de obtener los errores de entrenamiento y de prueba para cada número de épocas posible, se procede a graficar error v/s número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhhj/GKJxBhjjF8skRhjjPHL/wf+g8evZfzUjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58e23d0290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(epochs, train_errors)\n",
    "pyplot.plot(epochs, test_errors)\n",
    "pyplot.xlabel('Epochs')\n",
    "pyplot.ylabel('Error')\n",
    "pyplot.legend(['Train error', 'Test error'], loc='upper left')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este gráfico, se observa como el error de entrenamiento disminuye paulatinamente a medida que aumenta la cantidad de épocas, presentando sólo algunos leves incrementos en el camino. El error de entrenamiento más alto alcanzado fue del 7,5 %, aproximádamente (10 épocas), mientras que el mínimo se logra se logra para 100 épocas, siendo prácticamente del 0%. Esto resulta lógico si se considera que se está usando una baja tasa de aprendizaje, ya que de acuerdo a la teoría, es una buena práctica asignar valores pequeños a este parámetro cuando la cantidad de épocas especificadas es alta.\n",
    "\n",
    "La situación es radicalmente distinta para el error de pruebas: Presenta un comportamiento bastante particular, pues entre las 10 y 20 épocas, el error decrece, para luego mantenerse constante entre 20 y 35 épocas y presentar un comportamiento relativamente oscilatorio entre 35 y 100 épocas. Es importante notar que el error de pruebas siempre fue más alto que el de entrenamiento, siendo su valor máximo del 17,5% con 55 épocas, aproximádamente.\n",
    "\n",
    "Recordar que los pesos de las neuronas son actualizados en la fase de entrenamiento por medio de un método iterativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Entrenamiento vía *weight decay***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de regularizar los pesos de la red, es posible introducir un nuevo término en la función de actualización de estos, conocido como *weight decay*. Este parámetro penaliza a los pesos más grandes de la red, de tal forma que mientras mayor es el peso, mayor es la reducción que experimenta. Con esta práctica se espera evitar que el modelo no sufra de *overfitting*.\n",
    "\n",
    "Recordar, que hasta ahora, los pesos estaban siendo actualizados en base a la expresión:\n",
    "\n",
    "**peso_i = peso_i-1 + tasa_de_aprendizaje x error_i-1 x input**\n",
    "\n",
    "Al incluir el nuevo término, la expresión queda:\n",
    "\n",
    "**peso_i = peso_i-1 + tasa_de_aprendizaje x error_i-1 x input - tasa_de_aprendizaje x weight_decay * peso_i-1**\n",
    "\n",
    "El razonamiento es análogo para el bias.\n",
    "\n",
    "En esta sección, se introduce *weight_decay* dentro del proceso de entrenamiento de la red. Así, se vuelve a estudiar el problema de la sección 2.3 y el modelo es entrado nuevamente, para luego generar predicciones sobre los conjuntos de entrenamiento y de prueba. Se usará *weight_decay* = 0,01 en este caso (la tasa de aprendizaje y el número máximo de épocas se mantienen en 0,03 y 100, respectivamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se inicializa red\n",
    "random.seed(1)\n",
    "network = init_network(n_features, 10, n_classes)\n",
    "\n",
    "# Se almacenan errores para cada número de épocas posible\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Para cada cantidad de épocas posible...\n",
    "for i in range(10, max_epochs + 5, 5):\n",
    "    # ... se entrena la red...\n",
    "    train_network(network, train_dataset, learning_rate, i, n_classes, weight_decay=0.01)\n",
    "    # ... y se obtiene error promedio\n",
    "    train_error = predict_classes(network, train_dataset)\n",
    "    test_error = predict_classes(network, test_dataset)\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con ello, es interesante observar como varian las predicciones y los errores por medio de esta modificación. Se procede, entonces, a graficar error v/s número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt4VeWVx/HvygUSIZAAEZUkEm4qiEAMKFYt1htaR6yC\nQLVV1FKdsa211qEzfaqllxGn02qV2iJiYbSixdqipWJBp14QJVwVEAmIEIsQIiCgCCFr/jg7eAgJ\nJyTZOSc5v8/znId9efc+K+c5ZGXvd6/3NXdHRETkSFLiHYCIiCQ+JQsREYlJyUJERGJSshARkZiU\nLEREJCYlCxERiUnJQkREYlKyEBGRmJQsREQkprR4B9BUunTp4t27d493GCIiLcrixYu3uXturHat\nJll0796dkpKSeIchItKimNn79Wmn21AiIhKTkoWIiMSkZCEiIjEpWYiISExKFiIiEpOShYiIxKRk\nISIiMSV9stjxyT5+PX8tb5XtjHcoIiIJq9UU5TVUaorxq3nvUuVO/7yO8Q5HRCQhJf2VRVZGOn2O\nzWLpxh3xDkVEJGElfbIAKDoxm6Ubt1NV5fEORUQkISlZAIMKcvh4byXrt+2OdygiIglJyQIoKsgB\nYMn7uhUlIlIbJQugR5d2dMxMZ8nG7fEORUQkISlZACkpxqCCbCULEZE6KFkEBuXnsHbrbj7euz/e\noYiIJBwli0DRidm4wzI9QisicphQk4WZDTezNWZWamYTatl/rpktMbNKMxsZtX2gmb1uZivNbIWZ\njQ4zToCB+dmYoVtRIiK1CC1ZmFkqMBm4BOgLjDWzvjWabQSuB/5QY/snwNfdvR8wHLjPzLLDihU+\nL85boisLEZHDhHllMQQodff17r4PmAmMiG7g7hvcfQVQVWP7u+6+Nlj+J7AViDmheGOpOE9EpHZh\nJotuwKao9bJg21ExsyFAG2BdE8VVp0EFOezaW8m6chXniYhES+gObjM7HvhfYJy7V9Wyf7yZlZhZ\nSXl5eaPf72BxnvotREQOEWay+ADIj1rPC7bVi5l1AP4K/Ke7L6ytjbtPcfdidy/OzW38XaqDxXmq\n5BYROUSYyWIR0NvMCs2sDTAGmF2fA4P2zwAz3H1WiDEeQsV5IiK1Cy1ZuHslcCswF1gNPOXuK81s\nopldDmBmg82sDBgF/M7MVgaHXw2cC1xvZsuC18CwYo1WVBApztv5qYrzRESqhTr5kbvPAebU2Paj\nqOVFRG5P1TzuMeCxMGOrS3W/xfJNOzi3T+gPYImItAgJ3cEdDwPyO6o4T0SkBiWLGrIy0jmpq4rz\nRESiKVnUYlBBjorzRESiKFnUoqggW8V5IiJRlCxqMUjFeSIih1CyqIWK80REDqVkUQsV54mIHErJ\nog4qzhMR+ZySRR2qi/OWbdKtKBERJYs6HCzOe1+3okRElCzq8HlxnpKFiIiSxREMKshh2aYdKs4T\nkaSnZHEEKs4TEYlQsjiCohNVnCciAkoWR9SjSzuyj1FxnoiIksURmBmD8lWcJyKiZBGDivNEREJO\nFmY23MzWmFmpmU2oZf+5ZrbEzCrNbGSNfc+b2Q4zey7MGGOp7rdQcZ6IJLPQkoWZpQKTgUuAvsBY\nM+tbo9lG4HrgD7Wc4r+Br4UVX30NyM9WcZ6IJL0wryyGAKXuvt7d9wEzgRHRDdx9g7uvAKpqHuzu\n84FdIcZXL+3bpqk4T0SSXpjJohuwKWq9LNjW4qg4T0SSXYvu4Daz8WZWYmYl5eXlob1PdXFeqYrz\nRCRJhZksPgDyo9bzgm1Nxt2nuHuxuxfn5uY25akPcbA4T/0WIpKkwkwWi4DeZlZoZm2AMcDsEN8v\nNAeL89RvISJJKrRk4e6VwK3AXGA18JS7rzSziWZ2OYCZDTazMmAU8DszW1l9vJm9AvwRON/Myszs\n4rBijeXz4jw9PisiySktzJO7+xxgTo1tP4paXkTk9lRtx54TZmxHq6ggh5fWlLPzk/10PCY93uGI\niDSrFt3B3ZwOFueV6epCRJKPkkU9DcjPJkXFeSKSpJQs6ql92zT6qDhPRJKUksVRKDpRxXkikpyU\nLI5CUUGOivNEJCkpWRyFooJsQP0WIpJ8lCyOQmGXduSoOE9EkpCSxVEwMwYV5Kg4T0SSjpLFURqU\nn03p1t3s/EQz54lI8lCyOErVxXlLN+lWlIgkDyWLo3SwOE+3okQkiShZHKXq4ryl6uQWkSSiZNEA\nRSfmsGyjivNEJHkoWTRAUUEOuz6rZO1WFeeJSHJQsmiA6uI83YoSkWShZNEAKs4TkWQTarIws+Fm\ntsbMSs1sQi37zzWzJWZWaWYja+y7zszWBq/rwozzaKk4T0SSTWjJwsxSgcnAJUBfYKyZ9a3RbCNw\nPfCHGsd2Au4CzgCGAHeZWU5YsTZEUYGK80QkeYR5ZTEEKHX39e6+D5gJjIhu4O4b3H0FUFXj2IuB\nv7v7R+6+Hfg7MDzEWI9aUYGK80QkeYSZLLoBm6LWy4JtYR/bLFScJyLJpEV3cJvZeDMrMbOS8vLy\nZn3vdm3TOOm4DnoiSkSSQpjJ4gMgP2o9L9jWZMe6+xR3L3b34tzc3AYH2lBFBdkqzhORpBBmslgE\n9DazQjNrA4wBZtfz2LnARWaWE3RsXxRsSyiDVJwnIkkitGTh7pXArUR+ya8GnnL3lWY20cwuBzCz\nwWZWBowCfmdmK4NjPwJ+QiThLAImBtsSysGZ83QrSkRaubQwT+7uc4A5Nbb9KGp5EZFbTLUdOw2Y\nFmZ8jXWwOO/97YwdUhDvcEREQtOiO7jj7fPiPF1ZiEjrpmTRSEUF2awr38OOT/bFOxQRkdAoWTTS\n58V5qrcQkdZLyaKRqovzlqo4T0RaMSWLRlJxnogkAyWLJqDiPBFp7ZQsmoBmzhOR1k7Jogmc0aMT\nKQYzF22MdygiIqFQsmgCeTnHMHpwPo8tfJ8N2/bEOxwRkSanZNFEvnthH9JTU5j0/DvxDkVEpMkp\nWTSRY7MyuPmLPfnb2x9SsiHhhrESEWkUJYsmdNM5hXTt0Jaf/nU17noySkRaDyWLJnRMmzTuuOgk\nlm3awXMrNsc7HBGRJqNk0cSuLMrjlOM7MOn5d9i7/0C8wxERaRJKFk0sNcX4z0tPoWz7p8x4fUO8\nwxERaRJKFiE4u3cXzjsplwdeLGX7Ho1GKyItn5JFSH5w6Sns+ayS++evjXcoIiKNFmqyMLPhZrbG\nzErNbEIt+9ua2ZPB/jfMrHuwvY2ZPWpmb5nZcjMbFmacYejTNYvRgwt4bOH7vKdCPRFp4UJLFmaW\nCkwGLgH6AmPNrG+NZjcC2929F/ArYFKw/RsA7t4fuBD4HzNrcVdB372wN23TUpj0NxXqiUjLFuYv\n4CFAqbuvd/d9wExgRI02I4DpwfIs4HwzMyLJ5UUAd98K7ACKQ4w1FNWFes+v/JA331Ohnoi0XDGT\nhZmlmtkvGnDubsCmqPWyYFutbdy9EtgJdAaWA5ebWZqZFQKnA/kNiCHubjqnB8d1yOBnf12lIcxF\npMWKmSzc/QBwdjPEEm0akeRSAtwHLAAOK1ows/FmVmJmJeXl5c0cYv1ktknljotPYnnZTp57S4V6\nItIy1fc21FIzm21mXzOzK6tfMY75gEOvBvKCbbW2MbM0oCNQ4e6V7v5ddx/o7iOAbODdmm/g7lPc\nvdjdi3Nzc+v5ozS/Kwd1o+/xHZj0NxXqiUjLVN9kkQFUAF8C/iV4XRbjmEVAbzMrNLM2wBhgdo02\ns4HrguWRwIvu7mZ2jJm1AzCzC4FKd19Vz1gTTkqK8cMvn8IHOz5l+oIN8Q5HROSopdWnkbuPO9oT\nu3ulmd0KzAVSgWnuvtLMJgIl7j4beAT4XzMrBT4iklAAjgXmmlkVkauPrx3t+yeas3p14UsnH8uD\nL5UyqjifTu3axDskEZF6s/qMjmpmecADwBeCTa8A33H3shBjOyrFxcVeUlIS7zCOaO2WXQy//xW+\nduaJ3H15v3iHIyKCmS1295hPm9b3NtSjRG4ZnRC8ng22yVHo3TWLMcGMeuvLNV+3iLQc9U0Wue7+\naNDxXOnuvwcSt0c5gd12QR/apqVwjwr1RKQFqW+yqDCza4Oai1Qzu5ZIh7ccpdysttwyrCcvrNrC\nG+v1EYpIy1DfZHEDcDXwIbCZyJNLR93pLRE3nt2D4ztm8PM5q1WoJyItQr0quIEr3f1yd89192Pd\n/Qp339gM8bVKmW1SueOiSKHesyv+Ge9wRERiqm8F99hmiCWpfGVQN/qd0IF7n1+jQj0RSXj1vQ31\nmpk9aGbnmFlR9SvUyFq5lGBGvQ92fMqjr22IdzgiIkdUr6I8YGDw78SobU6kolsa6KxeXTj/5GP5\nzUulXF2cR+f2beMdkohIrerTZ5ECPOTu59V4KVE0gR9cejKf7D/ArzWjnogksPr0WVQBdzZDLEmp\n17FZjB2Sz+NvbGSdCvVEJEHVt89inpndYWb5Ztap+hVqZEnktgv6kJGeqkI9EUlY9U0Wo4F/A14G\nFgevxB6IqQXp0j5SqPf3VVs0Kq2IJKT6jjpbGHYgye6mcwpZunE7d81eydZde7njopOIzDArIhJ/\nR7yyMLM7o5ZH1dj387CCSkZt01L57bWnM2ZwPpNfWseds1aw/0BVvMMSEQFi34YaE7X8gxr7hjdx\nLEkvLTWF/7qyP98+vzd/XFzG+BklfLKvMt5hiYjETBZWx3Jt69IEzIzbL+zDT684lX+8W85XH36D\nj/bsi3dYIpLkYiULr2O5tnVpQteeeSK/ueZ0Vm3+mJEPLWDTR5/EOyQRSWKxksUAM/vYzHYBpwXL\n1ev9Y53czIab2RozKzWzCbXsb2tmTwb73zCz7sH2dDObbmZvmdlqM6t5CywpDD/1OB678Qy27f6M\nKx9awKp/fhzvkEQkSR0xWbh7qrt3cPcsd08LlqvX0490bDBa7WTgEqAvMNbM+tZodiOw3d17Ab8C\nJgXbRwFt3b0/cDrwzepEkmyGFHbijzefRaoZo3/3OgvWbYt3SCKShOpbZ9EQQ4BSd1/v7vuAmcCI\nGm1GANOD5VnA+RZ5XtSBdmaWBmQC+4Ck/bP6pOOy+NO/nkXXjhlcP20Rz2lYcxFpZmEmi27Apqj1\nsmBbrW3cvRLYCXQmkjj2EJloaSPwC3f/qOYbmNl4Mysxs5Ly8vKm/wkSyAnZmcy6eSin5XXkW08s\n5fevvRfvkEQkiYSZLBpjCHAAOAEoBL5nZj1qNnL3Ke5e7O7Fubmtf0rw7GPa8NhNZ3DBKV25+9lV\n3Pv8O7jrOQMRCV+YyeIDID9qPS/YVmub4JZTRyJze38VeN7d97v7VuA1oDjEWFuMjPRUHrqmiLFD\nCvjN/63jjj+qeE9EwhdmslgE9DazQjNrQ6TAb3aNNrOB64LlkcCLHvlTeSPBXBlm1g44E9Aoe4G0\n1BR+/pVTue2C3jy9pIxvqHhPREIWWrII+iBuBeYCq4Gn3H2lmU00s8uDZo8Anc2sFLgdqH68djLQ\n3sxWEkk6j7r7irBibYnMjNsu6MPPv9Kfl98tZ+zDb1Cx+7N4hyUirZS1lnvexcXFXlKSnAPhvrDy\nQ771xFJOyM5kxg1DyO90TLxDEpEWwswWu3vM2/xKFq1EyYaPuHF6CZUHqhJietYTOx/D/4wawLEd\nMuIdiogcgZJFEirduouHX36PfXHu8HZ3Xli1hU7t2jDjhiH0yG0f13hEpG5KFhJXK8p2MO7RRTgw\n7frBDMzPjndIIlKL+iaLRK2zkBbutLxsnr7lLNq3TWPslIW8tGZrvEMSkUZQspDQdO/Sjlm3DKVH\nbjtuml7CrMVl8Q5JRBpIyUJCdWxWBk9+cyhDe3Tmjj8u5zf/V6qqc5EWSMlCQte+bRrTrh/M5QNO\n4N7n1/DjZ1dRVaWEIdKSpMU7AEkObdJSuG/0QI7NasvUV9+jfPdn/PLqAbRNS413aCJSD0oW0mxS\nUowfXtaXrh0y+Nmc1VTs/owpXy+mQ8YRp0YRkQSg21DS7L5xbg/uGz2Qkg3bGf27hWz9eG+8QxKR\nGJQsJC6uGNSNadcPZmPFHr7ymwWsK98d75BE5AiULCRuzu2Ty8zxQ9m7/wAjH1rA0o3b4x2SiNRB\nyULiqn9eR56+5Sw6ZKbz1Yff4MV3tsQ7JBGphZKFxF33Lu2YdfNZ9Dq2Pd+YsZinSjbFPkhEmpWS\nhSSE3Ky2PDH+TM7q2Zk7Z61g8ksq3hNJJHp0VhJG+7ZpPHLdYO6ctZz/nruGBeu2kXNMm7jGNHTn\nXzn5k8WNPs8J2Zkc38KHa99bWcW7W3ZpGt8EtK9Dd4Z+475Q3yPUZGFmw4H7gVRgqrvfU2N/W2AG\ncDqRubdHu/sGM7sG+H5U09OAIndfFma8En9t0lL45dUDKeh0DM+9tZnNO+P3WG2KH+Ce3Q9xwFLY\naR0afJ6qKueTnc6O9m3JzmyZNSX7D1SxZede2lc5aakW73CkhvJmuAoPbYhyM0sF3gUuBMqITI86\n1t1XRbX5V+A0d7/ZzMYAX3H30TXO0x/4s7v3PNL7aYhyaXIbF8K0i2HUdOh3RYNPs3f/Ab4zcylz\nV27h5i/25N+Hn4RZy/mFWz3cfJU7064fzKCCnHiHJE0oEYYoHwKUuvt6d98HzARG1GgzApgeLM8C\nzrfD/xeNDY4VaV6l88FSoMcXG3WajPRUfnPN6VxzRgG//cc6vvfH5S3mVs4/3i1nzJSFZLZJ5elb\nzlKiSGJh3obqBkQ/1lIGnFFXG3evNLOdQGdgW1Sb0RyeZETCVzoPuhVDZuN/QaamGD+94lSO65DB\n//z9Xbbt3sdD1xTRrm3idhs+s7SM7/9xBb27ZjF93GBNkZvkEvppKDM7A/jE3d+uY/94Mysxs5Ly\n8vJmjk5atT0V8M+l0Ov8JjulmfGt83tzz5X9eXVtOV99eCEVuz9rsvM3FXdnysvr+O6TyxlS2Ikn\nv3mmEoWEmiw+APKj1vOCbbW2MbM0oCORju5qY4An6noDd5/i7sXuXpybm9skQYsAsP4lwKFn0yWL\namOGFDDla8Ws2bKLqx5awMaKT5r8PRqqqsr56V9X8/M573DZacfz6LjBGuhRgHCTxSKgt5kVmlkb\nIr/4Z9doMxu4LlgeCbzoQY+7maUAV6P+ComHdS9CRjZ0Kwrl9Bf07crjN53Jjk/3c+VDC3j7g52h\nvM/R+KzyAN95chmPvPoe477QnV+PGaQh5OWg0JKFu1cCtwJzgdXAU+6+0swmmtnlQbNHgM5mVgrc\nDkyIOsW5wCZ3Xx9WjCK1co8ki57nQUp4vyxPPzGHWTcPpW1aCmOmLOTVtdtiHxSSXXv3c8PvF/Hs\n8n8y4ZKT+dFlfUlJaTlPbEn4Qnt0trnp0VlpMltWwkNnweUPQtHXQn+7D3fu5fpH32Rd+W5+MWoA\nIwZ2C/09o23dtZfrpy3i3S27mHTVaVx1el6zvr/EVyI8OivSMpXOi/zb80vN8nbHdYzMUz6oIIfv\nzFzG1Fea72L6vW17uOqhBby3bQ9TrytWopA6KVmI1FQ6H3JPgY7N9xd+x8x0ZtwwhEtOPY6f/nU1\n/zVndejzlC/ftIOrHlrAns8O8MT4Mxl20rGhvp+0bEoWItH27YGNrzfpI7P1lZGeyoNfLeLrQ0/k\ndy+vD7V47//WbGXMlIW0axspthuYnx3K+0jrkbgVQSLxsOE1OLAvLskCIsV7P768H107ZPDfc9dQ\nsafpi/f+tKSMO2et4KTjsnh03GCOzVINhcSmKwuRaOvmQ1omFJwVtxDMjH87rxf3jjyN10q3Mfbh\nhWxrguI9d+e3/1jH7U8t54wenZg5/kwlCqk3XVmIRCudD92/AOnx/yV6dXE+Xdq34V8fX8JVDy3g\nmjMKMBr+OOs7H+7i6SVlXD7gBH4xagBt0vS3otSfkoVIte3vQ8VaKL4h3pEc9KWTu/KHb5zJ+BmL\n+fmcdxp1LjO46exC/uPSU1RDIUdNyUKk2rr5kX/j1F9Rl6KCHF7/wZf4rLJxnd2pZmS2UUW2NIyS\nhUi10vnQMR+69Il3JIdJT00hPVW3jSR+9O0TATiwH957OVKI14ImJhJpLkoWIgBlJfDZxwl3C0ok\nUShZiEBkiA9LhcLGzYon0lopWYhApHM7rxgyVcksUhslC5E9FfDPZaFMdCTSWihZiFTPitfrgnhH\nIpKwlCxESudDZg6cMDDekYgkLCULSW7ukf6KHuHOiifS0oWaLMxsuJmtMbNSM5tQy/62ZvZksP8N\nM+sete80M3vdzFaa2VtmFv/BeqT12fI27N6iR2ZFYggtWZhZKjAZuAToC4w1s741mt0IbHf3XsCv\ngEnBsWnAY8DN7t4PGAbsDytWSWKlwRAfzTQrnkhLFeaVxRCg1N3Xu/s+YCYwokabEcD0YHkWcL6Z\nGXARsMLdlwO4e4W7HwgxVklW6+bDsf2gwwnxjkQkoYWZLLoBm6LWy4JttbZx90pgJ9AZ6AO4mc01\nsyVmdmdtb2Bm482sxMxKysvLm/wHkFZu3x7YuBB66apCJJZE7eBOA84Grgn+/YqZHXZT2d2nuHux\nuxfn5uY2d4zS0m14NTIrnuorRGIKM1l8AORHrecF22ptE/RTdAQqiFyFvOzu29z9E2AOUBRirJKM\nSqtnxRsa70hEEl6YyWIR0NvMCs2sDTAGmF2jzWzgumB5JPCiuzswF+hvZscESeSLwKoQY5VkVDoP\nup+dELPiiSS60OazcPdKM7uVyC/+VGCau680s4lAibvPBh4B/tfMSoGPiCQU3H27mf2SSMJxYI67\n/zWsWCUJbd8AH62DId+IdyQS2L9/P2VlZezduzfeobRKGRkZ5OXlkZ6e3qDjQ538yN3nELmFFL3t\nR1HLe4FRdRz7GJHHZ0WaXvUjsxriI2GUlZWRlZVF9+7dMc0p0qTcnYqKCsrKyigsLGzQORK1g1sk\nXOtehI4F0LlXvCORwN69e+ncubMSRQjMjM6dOzfqqk3JQpLPgf2w/h+RR2b1iymhKFGEp7GfrZKF\nJJ9Nb8K+XXpkVg5RUVHBwIEDGThwIMcddxzdunU7uL5v3756nWPcuHGsWbMm5EjjI9Q+C5GEtG5+\nZFa8HpoVTz7XuXNnli1bBsDdd99N+/btueOOOw5p4+64Oykptf+d/eijjzZ5XJWVlaSlpdW5XpdY\nsR4tXVlI8imdD3mDIaNjvCORFqC0tJS+fftyzTXX0K9fPzZv3sz48eMpLi6mX79+TJw48WDbs88+\nm2XLllFZWUl2djYTJkxgwIABDB06lK1btx527t27d3P99dczZMgQBg0axLPPPgvA1KlTueKKKzjv\nvPO4+OKLmTdvHsOGDeOyyy6jf//+ANx7772ceuqpnHrqqTzwwAN1xtpUdGUhyWXPNti8HM77z3hH\nIkfw42dXsuqfHzfpOfue0IG7/qVfg4595513mDFjBsXFxQDcc889dOrUicrKSs477zxGjhxJ376H\njpO6c+dOvvjFL3LPPfdw++23M23aNCZMOHTw7YkTJzJ8+HB+//vfs337ds444wwuvPBCAJYuXcqy\nZcvIyclh3rx5lJSUsGrVKgoKCnjjjTd4/PHHWbRoEZWVlQwZMoRhw4aRmZl5WKxNRVcWklzWVc+K\np/GgpP569ux5yC/fJ554gqKiIoqKili9ejWrVh1eM5yZmckll1wCwOmnn86GDRsOa/PCCy/ws5/9\njIEDB3Leeeexd+9eNm7cCMBFF11ETk7OwbZDhw6loKAAgFdffZWrrrqKzMxMsrKyuOKKK3jllVdq\njbWp6MpCksu6+ZDZCY7XrHiJrKFXAGFp167dweW1a9dy//338+abb5Kdnc21115b6yOpbdq0Obic\nmppKZWXlYW3cnT//+c/07NnzkO0vv/zyIe9ZM4b6xtqUdGUhyaOqKtJf0VOz4knDffzxx2RlZdGh\nQwc2b97M3LlzG3yuiy+++GB/A0RuPdXHOeecwzPPPMOnn37K7t27+ctf/sI555zT4DjqQ1cWkjy2\nvA17tuqRWWmUoqIi+vbty8knn8yJJ57IF77whQaf66677uK2226jf//+VFVV0atXL/7yl7/EPG7I\nkCGMHTuWwYMHA3DLLbfQv39/SktLGxxLLBYZt6/lKy4u9pKSkniHIYns1V/BvLvhe2sg67h4RyM1\nrF69mlNOOSXeYbRqtX3GZrbY3WN2cug2lCSP0vnQ9VQlCpEGULKQ5PDZ7siseJprW6RBlCwkOWx4\nFar2Qy/1V4g0hJKFJIfSeZB+jGbFE2kgJQtJDuvmR2bFS2sb70hEWqRQk4WZDTezNWZWamYTatnf\n1syeDPa/YWbdg+3dzexTM1sWvH4bZpzSyn30Hny0XhMdiTRCaHUWZpYKTAYuBMqARWY2292j6+Jv\nBLa7ey8zGwNMAkYH+9a5u8pspfHWBbPiqb5CjqCiooLzz498Rz788ENSU1PJzc0F4M033zykIvtI\npk2bxqWXXspxx7Wup+7CLMobApS6+3oAM5sJjACik8UI4O5geRbwoGn2E2lqpS9CdgF07hm7rSSt\n+gxRXh/Tpk2jqKiowcmioUOS17ddQ4WZLLoBm6LWy4Az6mrj7pVmthPoHOwrNLOlwMfAD939lRBj\nldaqch+89zL0H6lZ8aTBpk+fzuTJk9m3bx9nnXUWDz74IFVVVYwbN45ly5bh7owfP56uXbuybNky\nRo8eTWZm5mFXJGvXruXWW29l27ZttGvXjqlTp9KnTx+uvfZasrKyWLx4McOGDaNNmzZs3LiRdevW\nUVhYyMMPP8zNN9/MkiVLSE9P57777uPcc89l6tSpPPfcc+zcuZOUlBTmz58f2meQqMN9bAYK3L3C\nzE4H/mxm/dz9kDGLzWw8MB44OBqjyCHKglnx9Mhsy/K3CfDhW017zuP6wyX3HPVhb7/9Ns888wwL\nFiwgLS2N8ePHM3PmTHr27Mm2bdt4661InDt27CA7O5sHHniABx98kIEDD7+LPn78eKZOnUrPnj15\n7bXXuPXWW3nhhRcA2Lx5MwsXLiQlJYUf/vCHvPPOO7z88stkZGQwadIk2rZty1tvvcXKlSu59NJL\nWbt2LXDmI58eAAAIX0lEQVToUOZhCjNZfADkR63nBdtqa1NmZmlAR6DCI2OQfAbg7ovNbB3QBzhk\nPA93nwJMgchwH2H8ENLClQaz4hWeG+9IpIWaN28eixYtOjjs96effkp+fj4XX3wxa9as4dvf/jZf\n/vKXueiii454nh07drBw4UKuuuqqg9uiR6IdNWrUIbPajRgxgoyMDCAyJPn3v/99APr168cJJ5xw\ncByomkOZhyXMZLEI6G1mhUSSwhjgqzXazAauA14HRgIvurubWS7wkbsfMLMeQG9gfYixSmu1bj7k\nn6FZ8VqaBlwBhMXdueGGG/jJT35y2L4VK1bwt7/9jcmTJ/P0008zZcqUI56nS5cuB/tFakq0Iclr\nCu3RWXevBG4F5gKrgafcfaWZTTSzy4NmjwCdzawUuB2ofrz2XGCFmS0j0vF9s7t/FFas0krtLo/M\niqeJjqQRLrjgAp566im2bdsGRJ6a2rhxI+Xl5bg7o0aNYuLEiSxZsgSArKwsdu3addh5cnJyOP74\n43nmmWcAqKqqYvny5fWK4ZxzzuHxxx8HIoMBbt68mV69ejXFj1dvofZZuPscYE6NbT+KWt4LjKrl\nuKeBp8OMTZLA+pci/+qRWWmE/v37c9ddd3HBBRdQVVVFeno6v/3tb0lNTeXGG2/E3TEzJk2aBMC4\nceO46aabau3gnjlzJrfccgt33303+/bt49prr2XAgAExY/jWt77FN7/5Tfr37096ejozZsyo96O8\nTUVDlH/yETx6SdMHJPG3e2vkCag7SiFFgxUkOg1RHr7GDFGeqE9DNZ+UVMg9Kd5RSBhyT4LeFytR\niDQBJYuMjnD1jHhHISKS0PQnl4iIxKRkISIJo7X0oSaixn62ShYikhAyMjKoqKhQwgiBu1NRUXGw\nyK8h1GchIgkhLy+PsrIyysvL4x1Kq5SRkUFeXl6Dj1eyEJGEkJ6eTmFhYbzDkDroNpSIiMSkZCEi\nIjEpWYiISEytZrgPMysH3o93HI3UBdgW7yASiD6PQ+nz+Jw+i0M15vM40d1zYzVqNcmiNTCzkvqM\n0ZIs9HkcSp/H5/RZHKo5Pg/dhhIRkZiULEREJCYli8RS9zRbyUmfx6H0eXxOn8WhQv881GchIiIx\n6cpCRERiUrKIEzPLN7OXzGyVma00s+8E2zuZ2d/NbG3wb068Y20uZpZqZkvN7LlgvdDM3jCzUjN7\n0syadx7JODKzbDObZWbvmNlqMxua5N+N7wb/T942syfMLCOZvh9mNs3MtprZ21Hbav0+WMSvg89l\nhZkVNUUMShbxUwl8z937AmcC/2ZmfYEJwHx37w3MD9aTxXeA1VHrk4BfuXsvYDtwY1yiio/7gefd\n/WRgAJHPJSm/G2bWDfg2UOzupwKpwBiS6/vxe2B4jW11fR8uAXoHr/HAQ00RgJJFnLj7ZndfEizv\nIvLLoBswApgeNJsOXBGfCJuXmeUBXwamBusGfAmYFTRJps+iI3Au8AiAu+9z9x0k6XcjkAZkmlka\ncAywmST6frj7y8BHNTbX9X0YAczwiIVAtpkd39gYlCwSgJl1BwYBbwBd3X1zsOtDoGucwmpu9wF3\nAlXBemdgh7tXButlRJJpMigEyoFHg9tyU82sHUn63XD3D4BfABuJJImdwGKS9/tRra7vQzdgU1S7\nJvlslCzizMzaA08Dt7n7x9H7PPKoWqt/XM3MLgO2uvvieMeSINKAIuAhdx8E7KHGLadk+W4ABPfi\nRxBJoicA7Tj8lkxSa47vg5JFHJlZOpFE8bi7/ynYvKX6kjH4d2u84mtGXwAuN7MNwEwitxfuJ3L5\nXD3nSh7wQXzCa3ZlQJm7vxGszyKSPJLxuwFwAfCeu5e7+37gT0S+M8n6/ahW1/fhAyA/ql2TfDZK\nFnES3JN/BFjt7r+M2jUbuC5Yvg74S3PH1tzc/Qfunufu3Yl0XL7o7tcALwEjg2ZJ8VkAuPuHwCYz\nOynYdD6wiiT8bgQ2Amea2THB/5vqzyMpvx9R6vo+zAa+HjwVdSawM+p2VYOpKC9OzOxs4BXgLT6/\nT/8fRPotngIKiIyie7W71+zYarXMbBhwh7tfZmY9iFxpdAKWAte6+2fxjK+5mNlAIp39bYD1wDgi\nf9wl5XfDzH4MjCbyFOFS4CYi9+GT4vthZk8Aw4iMLrsFuAv4M7V8H4KE+iCRW3WfAOPcvaTRMShZ\niIhILLoNJSIiMSlZiIhITEoWIiISk5KFiIjEpGQhIiIxKVmIxGBmB8xsWdSryQbwM7Pu0SOJiiSq\ntNhNRJLep+4+MN5BiMSTrixEGsjMNpjZvWb2lpm9aWa9gu3dzezFYC6B+WZWEGzvambPmNny4HVW\ncKpUM3s4mK/hBTPLDNp/O5jvZIWZzYzTjykCKFmI1EdmjdtQo6P27XT3/kQqZu8Ltj0ATHf304DH\ngV8H238N/MPdBxAZ62llsL03MNnd+wE7gKuC7ROAQcF5bg7rhxOpD1Vwi8RgZrvdvX0t2zcAX3L3\n9cGgkB+6e2cz2wYc7+77g+2b3b2LmZUDedFDUgTD0/89mMAGM/t3IN3df2pmzwO7iQzr8Gd33x3y\njypSJ11ZiDSO17F8NKLHMzrA532JXwYmE7kKWRQ1wqpIs1OyEGmc0VH/vh4sLyAyei7ANUQGjITI\n1Je3wMH5xjvWdVIzSwHy3f0l4N+BjsBhVzcizUV/qYjElmlmy6LWn3f36sdnc8xsBZGrg7HBtm8R\nmeXu+0RmvBsXbP8OMMXMbiRyBXELkZnfapMKPBYkFAN+HUytKhIX6rMQaaCgz6LY3bfFOxaRsOk2\nlIiIxKQrCxERiUlXFiIiEpOShYiIxKRkISIiMSlZiIhITEoWIiISk5KFiIjE9P9h/qgJrjV6DQAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58e2459e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(epochs, train_errors)\n",
    "pyplot.plot(epochs, test_errors)\n",
    "pyplot.xlabel('Epochs')\n",
    "pyplot.ylabel('Error')\n",
    "pyplot.legend(['Train error', 'Test error'], loc='lower right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico anterior muestra que el error de entrenamiento, al igual que en el caso anterior, disminuye a medida que aumenta la cantidad de épocas, siendo su valor mínimo del 6,5%, aproximadamente, para 60 y 65 épocas. Desde este último punto en adelante, el error se mantiene constante en ese valor. De esta manera, el error de entrenamiento aumenta con *weight decay* en comparación con el caso en que este parámetro no fue utilizado.\n",
    "\n",
    "Por otro lado, el error de pruebas presenta un valor constante del 5%, aproximadamente, entre 10 y 45 épocas, aumentando al 7% desde las 50 épocas en adelante y manteniendo dicho valor hasta alcanzar las 100 épocas. Con ello, se tiene que el error de pruebas presenta una importante disminución al utilizar *weight_decay* en comparación con el caso en que no se utiliza, existiendo un 10,5% de diferencias entre ambas cifras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
